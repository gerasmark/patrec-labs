{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import statistics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import torch.utils.data as data_utils\n",
    "import os\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pomegranate.distributions import Normal\n",
    "from pomegranate.gmm import GeneralMixtureModel\n",
    "from pomegranate.hmm import DenseHMM\n",
    "#from pomegranate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def parse_free_digits(directory):\n",
    "    # Parse relevant dataset info\n",
    "    files = glob(os.path.join(directory, \"*.wav\"))\n",
    "    fnames = [f.split(\"/\")[1].split(\".\")[0].split(\"_\") for f in files]\n",
    "    ids = [f[2] for f in fnames]\n",
    "    y = [int(f[0]) for f in fnames]\n",
    "    speakers = [f[1] for f in fnames]\n",
    "    _, Fs = librosa.core.load(files[0], sr=None)\n",
    "\n",
    "    def read_wav(f):\n",
    "        wav, _ = librosa.core.load(f, sr=None)\n",
    "\n",
    "        return wav\n",
    "\n",
    "    # Read all wavs\n",
    "    wavs = [read_wav(f) for f in files]\n",
    "\n",
    "    # Print dataset info\n",
    "    print(\"Total wavs: {}. Fs = {} Hz\".format(len(wavs), Fs))\n",
    "\n",
    "    return wavs, Fs, ids, y, speakers\n",
    "\n",
    "\n",
    "def extract_features(wavs, n_mfcc=6, Fs=8000):\n",
    "    # Extract MFCCs for all wavs\n",
    "    window = 30 * Fs // 1000\n",
    "    step = window // 2\n",
    "    frames = [\n",
    "        librosa.feature.mfcc(\n",
    "            y=wav, sr=Fs, n_fft=window, hop_length=window - step, n_mfcc=n_mfcc\n",
    "        ).T\n",
    "        for wav in tqdm(wavs, desc=\"Extracting mfcc features...\")\n",
    "    ]\n",
    "\n",
    "    print(\"Feature extraction completed with {} mfccs per frame\".format(n_mfcc))\n",
    "\n",
    "    return frames\n",
    "\n",
    "\n",
    "def split_free_digits(frames, ids, speakers, labels):\n",
    "    print(\"Splitting in train test split using the default dataset split\")\n",
    "    # Split to train-test\n",
    "    X_train, y_train, spk_train = [], [], []\n",
    "    X_test, y_test, spk_test = [], [], []\n",
    "    test_indices = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "\n",
    "    for idx, frame, label, spk in zip(ids, frames, labels, speakers):\n",
    "        if str(idx) in test_indices:\n",
    "            X_test.append(frame)\n",
    "            y_test.append(label)\n",
    "            spk_test.append(spk)\n",
    "        else:\n",
    "            X_train.append(frame)\n",
    "            y_train.append(label)\n",
    "            spk_train.append(spk)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, spk_train, spk_test\n",
    "\n",
    "\n",
    "def make_scale_fn(X_train):\n",
    "    # Standardize on train data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.concatenate(X_train))\n",
    "    print(\"Normalization will be performed using mean: {}\".format(scaler.mean_))\n",
    "    print(\"Normalization will be performed using std: {}\".format(scaler.scale_))\n",
    "\n",
    "    def scale(X):\n",
    "        scaled = []\n",
    "\n",
    "        for frames in X:\n",
    "            scaled.append(scaler.transform(frames))\n",
    "        return scaled\n",
    "\n",
    "    return scale\n",
    "\n",
    "\n",
    "def parser(directory, n_mfcc=6):\n",
    "    wavs, Fs, ids, y, speakers = parse_free_digits(directory)\n",
    "    frames = extract_features(wavs, n_mfcc=n_mfcc, Fs=Fs)\n",
    "    X_train, X_test, y_train, y_test, spk_train, spk_test = split_free_digits(\n",
    "        frames, ids, speakers, y\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, spk_train, spk_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wavs: 3000. Fs = 8000 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting mfcc features...: 100%|██████████| 3000/3000 [00:12<00:00, 241.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction completed with 6 mfccs per frame\n",
      "Splitting in train test split using the default dataset split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###Βήμα 9\n",
    "X_train, X_test, y_train, y_test, spk_train, spk_test = parser('recordings/') #parse recordings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If using all data to calculate normalization statistics\n",
      "Normalization will be performed using mean: [-517.82970067   62.3857955    18.81777176    9.58994408  -19.21332918\n",
      "  -10.9054417 ]\n",
      "Normalization will be performed using std: [152.29960089  51.98705829  36.71929108  29.63888661  24.80403283\n",
      "  23.39483933]\n",
      "If using X_train + X_dev to calculate normalization statistics\n",
      "Normalization will be performed using mean: [-517.77180304   62.41689972   18.86552787    9.61652008  -19.17346574\n",
      "  -10.77057825]\n",
      "Normalization will be performed using std: [152.46343541  51.98376561  36.72489087  29.65300818  24.84129996\n",
      "  23.30360999]\n",
      "If using X_train to calculate normalization statistics\n",
      "Normalization will be performed using mean: [-515.81248143   62.70110253   18.93207515    9.70706692  -19.38836838\n",
      "  -10.70239465]\n",
      "Normalization will be performed using std: [151.82794745  52.35051782  36.82451718  29.70388096  24.75182463\n",
      "  23.48335437]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train) #80% train and 20% test\n",
    "print(\"If using all data to calculate normalization statistics\")\n",
    "scale_fn = make_scale_fn(X_train + X_val + X_test)\n",
    "print(\"If using X_train + X_dev to calculate normalization statistics\")\n",
    "scale_fn = make_scale_fn(X_train + X_val)\n",
    "print(\"If using X_train to calculate normalization statistics\")\n",
    "scale_fn = make_scale_fn(X_train)\n",
    "X_train = scale_fn(X_train)\n",
    "X_dev = scale_fn(X_val)\n",
    "X_test = scale_fn(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train set:\n",
      "Number of samples for digit 0: 216\n",
      "Number of samples for digit 1: 216\n",
      "Number of samples for digit 2: 216\n",
      "Number of samples for digit 3: 216\n",
      "Number of samples for digit 4: 216\n",
      "Number of samples for digit 5: 216\n",
      "Number of samples for digit 6: 216\n",
      "Number of samples for digit 7: 216\n",
      "Number of samples for digit 8: 216\n",
      "Number of samples for digit 9: 216\n",
      "For validation set:\n",
      "Number of samples for digit 0: 54\n",
      "Number of samples for digit 1: 54\n",
      "Number of samples for digit 2: 54\n",
      "Number of samples for digit 3: 54\n",
      "Number of samples for digit 4: 54\n",
      "Number of samples for digit 5: 54\n",
      "Number of samples for digit 6: 54\n",
      "Number of samples for digit 7: 54\n",
      "Number of samples for digit 8: 54\n",
      "Number of samples for digit 9: 54\n",
      "For test set:\n",
      "Number of samples for digit 0: 30\n",
      "Number of samples for digit 1: 30\n",
      "Number of samples for digit 2: 30\n",
      "Number of samples for digit 3: 30\n",
      "Number of samples for digit 4: 30\n",
      "Number of samples for digit 5: 30\n",
      "Number of samples for digit 6: 30\n",
      "Number of samples for digit 7: 30\n",
      "Number of samples for digit 8: 30\n",
      "Number of samples for digit 9: 30\n"
     ]
    }
   ],
   "source": [
    "#check number of samples for digit for train set and test set and then print the results\n",
    "train_number = []\n",
    "test_number = []\n",
    "val_number = []\n",
    "y_t = np.array(y_train)\n",
    "y_te = np.array(y_test)\n",
    "y_v = np.array(y_val)\n",
    "for i in range(0,10):\n",
    "    train_number.append((np.where(y_t == i)[0]).shape[0])\n",
    "print(\"For train set:\")\n",
    "for i in range(len(train_number)):                          \n",
    "    print('Number of samples for digit {}:'.format(i),train_number[i])\n",
    "for i in range(0,10):\n",
    "    val_number.append((np.where(y_v == i)[0]).shape[0])\n",
    "print(\"For validation set:\")\n",
    "for i in range(len(val_number)):                          \n",
    "    print('Number of samples for digit {}:'.format(i),val_number[i])\n",
    "print(\"For test set:\")\n",
    "for i in range(0,10):\n",
    "    test_number.append((np.where(y_te == i)[0]).shape[0])\n",
    "for i in range(len(test_number)):                          \n",
    "    print('Number of samples for digit {}:'.format(i),test_number[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Βήμα 10\n",
    "n_states = 2  # the number of HMM states\n",
    "n_mixtures = 2  # the number of Gaussians\n",
    "gmm = True  # whether to use GMM or plain Gaussian\n",
    "covariance_type = \"diag\"  # Use diagonal covariange\n",
    "\n",
    "\n",
    "# Gather data separately for each digit\n",
    "def gather_in_dic(X, labels):\n",
    "    dic = {}\n",
    "    for dig in set(labels):\n",
    "        x = [X[i] for i in range(len(labels)) if labels[i] == dig]\n",
    "        lengths = [len(i) for i in x]\n",
    "        y = [dig for _ in range(len(x))]\n",
    "        dic[dig] = (x, lengths, y)\n",
    "    return dic\n",
    "\n",
    "train_dic = gather_in_dic(X_train, y_train)\n",
    "val_dic = gather_in_dic(X_val, y_val)\n",
    "test_dic = gather_in_dic(X_test, y_test)\n",
    "labels = list(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create GMM HMM model with the following parameters: X which will be one digit, states, Gaussian distributions, whether to use GMM or not\n",
    "class GMM_HMM:\n",
    "    def __init__(self, X, n_states=2, n_mixtures=2, gmm=True):\n",
    "        self.X = X                                                        \n",
    "        self.n_states = n_states                                         \n",
    "        self.n_mixtures = n_mixtures                                     \n",
    "        self.gmm = gmm \n",
    "        self.trans_matrix = np.zeros(self.n_states, self.n_states)\n",
    "        for i in range(self.n_states):\n",
    "            for j in range(self.n_states):\n",
    "                if i + 1 == j or i == j:\n",
    "                    self.trans_matrix[i, j] = 0.5\n",
    "        self.trans_matrix[self.n_states][self.n_states - 1] = 1\n",
    "        self.start = np.zeros(self.n_states)\n",
    "        self.start[0] = 1\n",
    "        self.end = np.zeros(self.n_states)\n",
    "        self.end[-1] = 1          \n",
    "        self.dists = []                                                 # list of probability distributions for the HMM states\n",
    "        for i in range(self.n_states):\n",
    "            a = GeneralMixtureModel.from_samples(Normal, self.n_mixtures, np.float_(self.X))\n",
    "            self.dists.append(a)\n",
    "        self.model = DenseHMM.from_matrix(self.trans_matrix, self.dists, self.starts, self.ends, state_names=['s{}'.format(i) for i in range(n_states)]) \n",
    "      \n",
    "    def fit(self, X, max_iters=4):\n",
    "        self.max_iters = max_iters\n",
    "        self.model.fit(X, max_iters=self.max_iters) #train hmm model\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prediction, _ = self.model.viterbi(X) # run viterbi to predict \n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize one gmm-hmm for every digit\n",
    "def initialize_gmm_hmm(train_dic, n_states, n_mixtures):\n",
    "    hmm_list = []\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
