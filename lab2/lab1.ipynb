{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import statistics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import torch.utils.data as data_utils\n",
    "import os\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def parse_free_digits(directory):\n",
    "    # Parse relevant dataset info\n",
    "    files = glob(os.path.join(directory, \"*.wav\"))\n",
    "    fnames = [f.split(\"/\")[1].split(\".\")[0].split(\"_\") for f in files]\n",
    "    ids = [f[2] for f in fnames]\n",
    "    y = [int(f[0]) for f in fnames]\n",
    "    speakers = [f[1] for f in fnames]\n",
    "    _, Fs = librosa.core.load(files[0], sr=None)\n",
    "\n",
    "    def read_wav(f):\n",
    "        wav, _ = librosa.core.load(f, sr=None)\n",
    "\n",
    "        return wav\n",
    "\n",
    "    # Read all wavs\n",
    "    wavs = [read_wav(f) for f in files]\n",
    "\n",
    "    # Print dataset info\n",
    "    print(\"Total wavs: {}. Fs = {} Hz\".format(len(wavs), Fs))\n",
    "\n",
    "    return wavs, Fs, ids, y, speakers\n",
    "\n",
    "\n",
    "def extract_features(wavs, n_mfcc=6, Fs=8000):\n",
    "    # Extract MFCCs for all wavs\n",
    "    window = 30 * Fs // 1000\n",
    "    step = window // 2\n",
    "    frames = [\n",
    "        librosa.feature.mfcc(\n",
    "            y=wav, sr=Fs, n_fft=window, hop_length=window - step, n_mfcc=n_mfcc\n",
    "        ).T\n",
    "        for wav in tqdm(wavs, desc=\"Extracting mfcc features...\")\n",
    "    ]\n",
    "\n",
    "    print(\"Feature extraction completed with {} mfccs per frame\".format(n_mfcc))\n",
    "\n",
    "    return frames\n",
    "\n",
    "\n",
    "def split_free_digits(frames, ids, speakers, labels):\n",
    "    print(\"Splitting in train test split using the default dataset split\")\n",
    "    # Split to train-test\n",
    "    X_train, y_train, spk_train = [], [], []\n",
    "    X_test, y_test, spk_test = [], [], []\n",
    "    test_indices = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "\n",
    "    for idx, frame, label, spk in zip(ids, frames, labels, speakers):\n",
    "        if str(idx) in test_indices:\n",
    "            X_test.append(frame)\n",
    "            y_test.append(label)\n",
    "            spk_test.append(spk)\n",
    "        else:\n",
    "            X_train.append(frame)\n",
    "            y_train.append(label)\n",
    "            spk_train.append(spk)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, spk_train, spk_test\n",
    "\n",
    "\n",
    "def make_scale_fn(X_train):\n",
    "    # Standardize on train data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.concatenate(X_train))\n",
    "    print(\"Normalization will be performed using mean: {}\".format(scaler.mean_))\n",
    "    print(\"Normalization will be performed using std: {}\".format(scaler.scale_))\n",
    "\n",
    "    def scale(X):\n",
    "        scaled = []\n",
    "\n",
    "        for frames in X:\n",
    "            scaled.append(scaler.transform(frames))\n",
    "        return scaled\n",
    "\n",
    "    return scale\n",
    "\n",
    "\n",
    "def parser(directory, n_mfcc=6):\n",
    "    wavs, Fs, ids, y, speakers = parse_free_digits(directory)\n",
    "    frames = extract_features(wavs, n_mfcc=n_mfcc, Fs=Fs)\n",
    "    X_train, X_test, y_train, y_test, spk_train, spk_test = split_free_digits(\n",
    "        frames, ids, speakers, y\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, spk_train, spk_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wavs: 3000. Fs = 8000 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting mfcc features...: 100%|██████████| 3000/3000 [00:06<00:00, 435.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction completed with 6 mfccs per frame\n",
      "Splitting in train test split using the default dataset split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###Βήμα 9\n",
    "X_train, X_test, y_train, y_test, spk_train, spk_test = parser('recordings/') #parse recordings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If using all data to calculate normalization statistics\n",
      "Normalization will be performed using mean: [-517.82970067   62.3857955    18.81777176    9.58994408  -19.21332918\n",
      "  -10.9054417 ]\n",
      "Normalization will be performed using std: [152.29960089  51.98705829  36.71929108  29.63888661  24.80403283\n",
      "  23.39483933]\n",
      "If using X_train + X_dev to calculate normalization statistics\n",
      "Normalization will be performed using mean: [-517.77180304   62.41689972   18.86552787    9.61652008  -19.17346574\n",
      "  -10.77057825]\n",
      "Normalization will be performed using std: [152.46343541  51.98376561  36.72489087  29.65300818  24.84129996\n",
      "  23.30360999]\n",
      "If using X_train to calculate normalization statistics\n",
      "Normalization will be performed using mean: [-516.76986827   62.32102779   18.69007933    9.57460784  -19.35101183\n",
      "  -11.04056045]\n",
      "Normalization will be performed using std: [151.40761838  52.10330368  36.65275522  29.60096666  24.85887974\n",
      "  23.38236953]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train) #80% train and 20% test\n",
    "print(\"If using all data to calculate normalization statistics\")\n",
    "scale_fn = make_scale_fn(X_train + X_dev + X_test)\n",
    "print(\"If using X_train + X_dev to calculate normalization statistics\")\n",
    "scale_fn = make_scale_fn(X_train + X_dev)\n",
    "print(\"If using X_train to calculate normalization statistics\")\n",
    "scale_fn = make_scale_fn(X_train)\n",
    "X_train = scale_fn(X_train)\n",
    "X_dev = scale_fn(X_dev)\n",
    "X_test = scale_fn(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train set:\n",
      "Number of samples for digit 0: 216\n",
      "Number of samples for digit 1: 216\n",
      "Number of samples for digit 2: 216\n",
      "Number of samples for digit 3: 216\n",
      "Number of samples for digit 4: 216\n",
      "Number of samples for digit 5: 216\n",
      "Number of samples for digit 6: 216\n",
      "Number of samples for digit 7: 216\n",
      "Number of samples for digit 8: 216\n",
      "Number of samples for digit 9: 216\n",
      "For test set:\n",
      "Number of samples for digit 0: 30\n",
      "Number of samples for digit 1: 30\n",
      "Number of samples for digit 2: 30\n",
      "Number of samples for digit 3: 30\n",
      "Number of samples for digit 4: 30\n",
      "Number of samples for digit 5: 30\n",
      "Number of samples for digit 6: 30\n",
      "Number of samples for digit 7: 30\n",
      "Number of samples for digit 8: 30\n",
      "Number of samples for digit 9: 30\n"
     ]
    }
   ],
   "source": [
    "#check number of samples for digit for train set and test set and then print the results\n",
    "train_number = []\n",
    "test_number = []\n",
    "y_t = np.array(y_train)\n",
    "y_te = np.array(y_test)\n",
    "for i in range(0,10):\n",
    "    train_number.append((np.where(y_t == i)[0]).shape[0])\n",
    "print(\"For train set:\")\n",
    "for i in range(len(train_number)):                          \n",
    "    print('Number of samples for digit {}:'.format(i),train_number[i])\n",
    "print(\"For test set:\")\n",
    "for i in range(0,10):\n",
    "    test_number.append((np.where(y_te == i)[0]).shape[0])\n",
    "for i in range(len(test_number)):                          \n",
    "    print('Number of samples for digit {}:'.format(i),test_number[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Βήμα 10\n",
    "from pomegranate.distributions import Normal\n",
    "from pomegranate.gmm import GeneralMixtureModel\n",
    "from pomegranate.hmm import DenseHMM\n",
    "from pomegranate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create GMM HMM model with the following parameters: X which will be one digit, states, Gaussian distributions, whether to use GMM or not\n",
    "class GMM_HMM:\n",
    "    def __init__(self, X, n_states=4, n_mixtures=4, gmm=True):\n",
    "        self.X = X                                                        \n",
    "        self.n_states = n_states                                         \n",
    "        self.n_mixtures = n_mixtures                                     \n",
    "        self.gmm = gmm \n",
    "        self.max_iterations = 5\n",
    "        self.trans_matrix = np.array([[0.5, 0.5, 0, 0],\n",
    "                          [0, 0.5, 0.5, 0], \n",
    "                          [0, 0, 0.5, 0.5], \n",
    "                          [0, 0, 0, 1.0]]) \n",
    "        self.start = np.array([1.0, 0, 0, 0])\n",
    "        self.dists = []                                                 # list of probability distributions for the HMM states\n",
    "        for i in range(self.n_states):\n",
    "              if self.gmm:\n",
    "                  a = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, self.n_mixtures, np.float_(self.X))\n",
    "              else:\n",
    "                  a = MultivariateGaussianDistribution.from_samples(X)\n",
    "              self.dists.append(a)\n",
    "        self.model = HiddenMarkovModel.from_matrix(self.trans_matrix, self.dists, self.starts, self.ends, state_names=['s{}'.format(i) for i in range(n_states)]) \n",
    "      \n",
    "    def fit(self, X):\n",
    "        self.model.fit(X, max_iterations=self.max_iterations)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        logp, _ = self.model.viterbi(X)\n",
    "        return logp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
